# Environment Variables Configuration

# Copy this file to .env.local for local development
# Copy this file to .env.production for VPS deployment

# =============================================================================
# LLM PROVIDER API KEYS
# =============================================================================
# The system will try providers in the order specified by LLM_PROVIDER_ORDER
# Only configured providers (with valid API keys) will be used

# Groq API (FREE TIER - Recommended as first choice)
# Fast inference with Llama 3.3 70B model
# Get your key at: https://console.groq.com/keys
GROQ_API_KEY=your_groq_api_key_here

# Together AI (FREE TIER)
# Multiple open-source models available
# Get your key at: https://api.together.xyz/settings/api-keys
TOGETHER_API_KEY=your_together_api_key_here

# Hugging Face Inference API (FREE TIER)
# Access to many models via inference API
# Get your key at: https://huggingface.co/settings/tokens
HUGGINGFACE_API_KEY=your_huggingface_token_here

# Cohere API (FREE TIER)
# Command models with good performance
# Get your key at: https://dashboard.cohere.com/api-keys
COHERE_API_KEY=your_cohere_api_key_here

# OpenRouter (PAID - Fallback only)
# Used as last resort when free providers fail
# Get your key at: https://openrouter.ai/keys
OPENROUTER_API_KEY=your_openrouter_api_key_here

# =============================================================================
# LLM PROVIDER CONFIGURATION
# =============================================================================

# Order of providers to try (comma-separated, no spaces)
# Recommendation: Use free providers first, paid as fallback
# Default: groq,together,huggingface,cohere,openrouter
LLM_PROVIDER_ORDER=groq,together,huggingface,cohere,openrouter

# =============================================================================
# DEPLOYMENT CONFIGURATION
# =============================================================================

# Environment mode (development, staging, production)
NODE_ENV=development

# Base URL for the application
# Local: http://localhost:3000
# Staging: https://staging.aibreeds-demo.com
# Production: https://aibreeds-demo.com
NEXT_PUBLIC_API_URL=http://localhost:3000

# =============================================================================
# CACHE CONFIGURATION
# =============================================================================

# Cache expiration in days (default: 7)
CACHE_EXPIRATION_DAYS=7

# Maximum cache size in MB (default: 1000 for production, 500 for staging)
MAX_CACHE_SIZE_MB=1000

# =============================================================================
# SECURITY SETTINGS (Production Only)
# =============================================================================

# Rate limiting (requests per minute per IP)
RATE_LIMIT_RPM=60

# Enable CORS (for API access from other domains)
ENABLE_CORS=false

# Allowed origins (comma-separated, only if CORS enabled)
ALLOWED_ORIGINS=https://aibreeds-demo.com

# =============================================================================
# VPS DEPLOYMENT CONFIGURATION (For deployment scripts)
# =============================================================================

# VPS connection details
# VPS_HOST=your-vps-ip-or-domain
# VPS_USER=root
# VPS_PATH=/opt/aibreeds

# =============================================================================
# OPTIONAL: EXTERNAL API KEYS
# =============================================================================

# The Cat API (optional - for better cat breed images)
# Get your key at: https://thecatapi.com/signup
# CAT_API_KEY=your_cat_api_key_here

# Unsplash API (optional - for high-quality fallback images)
# Get your key at: https://unsplash.com/developers
# UNSPLASH_ACCESS_KEY=your_unsplash_key_here

# =============================================================================
# NOTES
# =============================================================================

# IMPORTANT FOR HONG KONG USERS:
# - OpenAI and Claude APIs may be blocked in Hong Kong
# - Groq, Together AI, and Hugging Face typically work well from HK
# - Test each provider individually to confirm accessibility
# - The system will automatically skip blocked/failed providers

# SECURITY:
# - NEVER commit .env.local or .env.production to Git
# - Keep your API keys secure and rotate them regularly
# - Use different keys for development and production

# TESTING:
# - You can test with just one provider configured
# - The system will warn if no providers are available
# - Check server logs to see which provider is being used

# RATE LIMITS (Free Tiers as of Jan 2026):
# - Groq: 30 requests/minute, 14,400/day
# - Together AI: Varies by model
# - Hugging Face: Rate limited but generous
# - Cohere: 100 requests/minute (trial)
# - OpenRouter: Pay-per-use

# =============================================================================
# AI IMAGE GENERATION (OPTIONAL)
# =============================================================================
# Used as fallback when breed image APIs fail or return poor matches
# These are premium services - you'll be charged per image generated

# OpenAI DALL-E 3 (Best Quality - Recommended)
# Cost: $0.040 per image (standard), $0.080 per image (HD)
# Get your key at: https://platform.openai.com/api-keys
OPENAI_API_KEY=your_openai_api_key_here

# Replicate (Stable Diffusion XL)
# Cost: ~$0.0025 per image (very affordable)
# Get your token at: https://replicate.com/account/api-tokens
REPLICATE_API_TOKEN=your_replicate_token_here

# Note: Together AI can also generate images using their SDXL endpoint
# No additional key needed if you already have TOGETHER_API_KEY above

# =============================================================================
# QUICK START
# =============================================================================

# 1. Copy this file:
#    cp .env.example .env.local
#
# 2. Sign up for at least one free LLM provider (Groq recommended)
#
# 3. Add your API key(s) above
#
# 4. (Optional) Add AI image generation keys for custom breed images
#
# 5. Run the development server:
#    npm run dev
#
# 6. Test the chatbot at http://localhost:3000
#
# 7. Check server logs to see which provider is being used
